"""
Base Intelligent Agent - Foundation for Phase 4 Specialized Agents

This module provides the base class for all Phase 4 intelligent agents that leverage
the Three-Engine Architecture for advanced problem-solving capabilities.
"""

import asyncio
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Union
from enum import Enum

from ..engines.perfect_recall_engine import PerfectRecallEngine
from ..engines.parallel_mind_engine import ParallelMindEngine  
from ..engines.creative_engine import CreativeEngine
from ..core.framework import ThreeEngineArchitecture


class ProblemComplexity(Enum):
    """Problem complexity levels for intelligent routing"""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    EXPERT = "expert"


class AgentCapability(Enum):
    """Agent capability types"""
    CODE_ANALYSIS = "code_analysis"
    DEBUG_DETECTION = "debug_detection"
    ARCHITECTURE_ADVISORY = "architecture_advisory"
    PERFORMANCE_OPTIMIZATION = "performance_optimization"
    SECURITY_AUDITING = "security_auditing"


@dataclass
class Problem:
    """Represents a problem to be solved by an intelligent agent"""
    description: str
    context: Dict[str, Any]
    complexity: ProblemComplexity
    priority: int = 1
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class AnalysisResult:
    """Result of problem analysis"""
    problem_type: str
    complexity_assessment: ProblemComplexity
    required_engines: List[str]
    confidence_score: float
    analysis_details: Dict[str, Any]
    recommendations: List[str]


@dataclass
class Solution:
    """Represents a solution generated by an intelligent agent"""
    solution_id: str
    approach: str
    implementation_steps: List[str]
    confidence_score: float
    estimated_effort: str
    risks: List[str]
    benefits: List[str]
    code_changes: Optional[Dict[str, str]] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ExecutionResult:
    """Result of solution execution"""
    success: bool
    execution_time: float
    output: Any
    errors: List[str]
    warnings: List[str]
    metrics: Dict[str, Any]
    feedback_score: Optional[float] = None


class IntelligentAgent(ABC):
    """
    Base class for all Phase 4 intelligent agents.
    
    Provides integration with the Three-Engine Architecture and common
    functionality for advanced problem-solving workflows.
    """
    
    def __init__(self, engines: ThreeEngineArchitecture, agent_id: str):
        self.engines = engines
        self.agent_id = agent_id
        self.logger = logging.getLogger(f"agent.{agent_id}")
        
        # Engine references for direct access
        self.perfect_recall = engines.perfect_recall
        self.parallel_mind = engines.parallel_mind
        self.creative_engine = engines.creative_engine
        self.coordinator = engines.coordinator
        
        # Agent state
        self.is_initialized = False
        self.learning_data = {}
        self.performance_metrics = {
            "problems_solved": 0,
            "success_rate": 0.0,
            "average_confidence": 0.0,
            "learning_score": 0.0
        }
    
    @property
    @abstractmethod
    def capabilities(self) -> List[AgentCapability]:
        """Return the capabilities this agent provides"""
        pass
    
    @property
    @abstractmethod
    def specialization(self) -> str:
        """Return the agent's specialization description"""
        pass
    
    async def initialize(self) -> bool:
        """Initialize the agent and its engine connections"""
        try:
            self.logger.info(f"Initializing {self.agent_id} agent...")
            
            # Ensure engines are initialized
            if not await self.engines.initialize():
                raise Exception("Failed to initialize Three-Engine Architecture")
            
            # Load agent-specific learning data
            await self._load_learning_data()
            
            # Initialize agent-specific components
            await self._initialize_agent_components()
            
            self.is_initialized = True
            self.logger.info(f"{self.agent_id} agent initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize {self.agent_id} agent: {e}")
            return False
    
    async def analyze_problem(self, problem: Problem) -> AnalysisResult:
        """
        Analyze a problem using the Three-Engine Architecture.
        
        This method coordinates between engines to provide comprehensive
        problem analysis with context, parallel processing, and creative insights.
        """
        if not self.is_initialized:
            raise RuntimeError(f"Agent {self.agent_id} not initialized")
        
        self.logger.info(f"Analyzing problem: {problem.description[:100]}...")
        
        try:
            # 1. Use Perfect Recall for context analysis
            context_analysis = await self.perfect_recall.analyze_context({
                "problem": problem.description,
                "context": problem.context,
                "agent_id": self.agent_id
            })
            
            # 2. Use Parallel Mind for multi-perspective analysis
            parallel_tasks = [
                self._analyze_complexity(problem),
                self._analyze_requirements(problem),
                self._analyze_constraints(problem)
            ]
            
            parallel_results = await self.parallel_mind.execute_parallel_tasks(parallel_tasks)
            
            # 3. Use Creative Engine for innovative analysis approaches
            creative_insights = await self.creative_engine.generate_insights({
                "problem": problem.description,
                "context": context_analysis,
                "parallel_analysis": parallel_results
            })
            
            # 4. Synthesize results
            analysis_result = await self._synthesize_analysis(
                problem, context_analysis, parallel_results, creative_insights
            )
            
            # 5. Update learning data
            await self._update_learning_data("analysis", analysis_result)
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"Problem analysis failed: {e}")
            raise
    
    async def generate_solution(self, analysis: AnalysisResult) -> List[Solution]:
        """
        Generate solutions based on analysis results.
        
        Uses the Creative Engine to generate multiple innovative solutions
        while leveraging Perfect Recall for context and Parallel Mind for
        concurrent solution development.
        """
        self.logger.info(f"Generating solutions for {analysis.problem_type}")
        
        try:
            # 1. Retrieve relevant context and patterns
            context = await self.perfect_recall.retrieve_relevant_context({
                "problem_type": analysis.problem_type,
                "complexity": analysis.complexity_assessment.value,
                "agent_id": self.agent_id
            })
            
            # 2. Generate multiple solution approaches in parallel
            solution_tasks = []
            for i in range(3, 6):  # Generate 3-5 solutions as per Phase 4 spec
                solution_tasks.append(
                    self._generate_single_solution(analysis, context, approach_id=i)
                )
            
            parallel_solutions = await self.parallel_mind.execute_parallel_tasks(solution_tasks)
            
            # 3. Use Creative Engine to enhance and diversify solutions
            enhanced_solutions = await self.creative_engine.enhance_solutions({
                "base_solutions": parallel_solutions,
                "analysis": analysis,
                "context": context
            })
            
            # 4. Rank and optimize solutions
            ranked_solutions = await self._rank_solutions(enhanced_solutions, analysis)
            
            # 5. Update learning data
            await self._update_learning_data("solution_generation", ranked_solutions)
            
            return ranked_solutions
            
        except Exception as e:
            self.logger.error(f"Solution generation failed: {e}")
            raise
    
    async def execute_solution(self, solution: Solution, context: Dict[str, Any]) -> ExecutionResult:
        """
        Execute a solution with intelligent monitoring and adaptation.
        
        Coordinates execution across engines with real-time monitoring,
        error handling, and adaptive optimization.
        """
        self.logger.info(f"Executing solution: {solution.solution_id}")
        
        try:
            # 1. Prepare execution context
            execution_context = await self._prepare_execution_context(solution, context)
            
            # 2. Execute with parallel monitoring
            execution_tasks = [
                self._execute_solution_steps(solution, execution_context),
                self._monitor_execution_progress(solution.solution_id),
                self._collect_execution_metrics(solution.solution_id)
            ]
            
            execution_results = await self.parallel_mind.execute_parallel_tasks(execution_tasks)
            
            # 3. Process results and handle any issues
            result = await self._process_execution_results(execution_results, solution)
            
            # 4. Update performance metrics
            await self._update_performance_metrics(result)
            
            # 5. Store execution data for learning
            await self._store_execution_data(solution, result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Solution execution failed: {e}")
            return ExecutionResult(
                success=False,
                execution_time=0.0,
                output=None,
                errors=[str(e)],
                warnings=[],
                metrics={}
            )
    
    async def learn_from_feedback(self, solution: Solution, result: ExecutionResult, 
                                 user_feedback: Dict[str, Any]) -> None:
        """
        Learn from execution results and user feedback.
        
        Updates the agent's learning model based on solution effectiveness
        and user satisfaction to improve future performance.
        """
        self.logger.info(f"Learning from feedback for solution: {solution.solution_id}")
        
        try:
            # 1. Analyze feedback patterns
            feedback_analysis = await self._analyze_feedback(solution, result, user_feedback)
            
            # 2. Update learning model
            await self._update_learning_model(feedback_analysis)
            
            # 3. Store feedback for future reference
            await self.perfect_recall.store_feedback({
                "solution_id": solution.solution_id,
                "result": result,
                "feedback": user_feedback,
                "analysis": feedback_analysis,
                "agent_id": self.agent_id
            })
            
            # 4. Update performance metrics
            self.performance_metrics["learning_score"] = await self._calculate_learning_score()
            
        except Exception as e:
            self.logger.error(f"Learning from feedback failed: {e}")
    
    # Abstract methods for agent-specific implementation
    @abstractmethod
    async def _initialize_agent_components(self) -> None:
        """Initialize agent-specific components"""
        pass
    
    @abstractmethod
    async def _analyze_complexity(self, problem: Problem) -> Dict[str, Any]:
        """Analyze problem complexity (agent-specific)"""
        pass
    
    @abstractmethod
    async def _generate_single_solution(self, analysis: AnalysisResult, 
                                       context: Dict[str, Any], approach_id: int) -> Solution:
        """Generate a single solution approach (agent-specific)"""
        pass
    
    @abstractmethod
    async def _execute_solution_steps(self, solution: Solution, 
                                     context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute solution steps (agent-specific)"""
        pass
    
    # Common helper methods
    async def _load_learning_data(self) -> None:
        """Load agent learning data from Perfect Recall"""
        try:
            self.learning_data = await self.perfect_recall.retrieve_agent_data(self.agent_id)
        except Exception as e:
            self.logger.warning(f"Could not load learning data: {e}")
            self.learning_data = {}
    
    async def _analyze_requirements(self, problem: Problem) -> Dict[str, Any]:
        """Analyze problem requirements"""
        return {
            "functional_requirements": [],
            "non_functional_requirements": [],
            "constraints": [],
            "dependencies": []
        }
    
    async def _analyze_constraints(self, problem: Problem) -> Dict[str, Any]:
        """Analyze problem constraints"""
        return {
            "technical_constraints": [],
            "resource_constraints": [],
            "time_constraints": [],
            "business_constraints": []
        }
    
    async def _synthesize_analysis(self, problem: Problem, context_analysis: Dict[str, Any],
                                  parallel_results: List[Dict[str, Any]], 
                                  creative_insights: Dict[str, Any]) -> AnalysisResult:
        """Synthesize analysis results from all engines"""
        return AnalysisResult(
            problem_type=self._determine_problem_type(problem, context_analysis),
            complexity_assessment=self._assess_complexity(parallel_results),
            required_engines=self._determine_required_engines(creative_insights),
            confidence_score=self._calculate_confidence_score(parallel_results),
            analysis_details={
                "context": context_analysis,
                "parallel_analysis": parallel_results,
                "creative_insights": creative_insights
            },
            recommendations=self._generate_recommendations(creative_insights)
        )
    
    async def _rank_solutions(self, solutions: List[Solution], 
                             analysis: AnalysisResult) -> List[Solution]:
        """Rank solutions by effectiveness and feasibility"""
        # Sort by confidence score and estimated effort
        return sorted(solutions, key=lambda s: (s.confidence_score, -len(s.risks)), reverse=True)
    
    async def _prepare_execution_context(self, solution: Solution, 
                                        context: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare execution context"""
        return {
            "solution": solution,
            "environment": context,
            "agent_id": self.agent_id,
            "timestamp": asyncio.get_event_loop().time()
        }
    
    async def _monitor_execution_progress(self, solution_id: str) -> Dict[str, Any]:
        """Monitor execution progress"""
        return {"solution_id": solution_id, "status": "monitoring"}
    
    async def _collect_execution_metrics(self, solution_id: str) -> Dict[str, Any]:
        """Collect execution metrics"""
        return {"solution_id": solution_id, "metrics": {}}
    
    async def _process_execution_results(self, execution_results: List[Dict[str, Any]], 
                                        solution: Solution) -> ExecutionResult:
        """Process execution results"""
        return ExecutionResult(
            success=True,
            execution_time=1.0,
            output=execution_results[0] if execution_results else None,
            errors=[],
            warnings=[],
            metrics=execution_results[2] if len(execution_results) > 2 else {}
        )
    
    async def _update_performance_metrics(self, result: ExecutionResult) -> None:
        """Update agent performance metrics"""
        self.performance_metrics["problems_solved"] += 1
        if result.success:
            current_rate = self.performance_metrics["success_rate"]
            total_problems = self.performance_metrics["problems_solved"]
            self.performance_metrics["success_rate"] = (
                (current_rate * (total_problems - 1) + 1.0) / total_problems
            )
    
    async def _store_execution_data(self, solution: Solution, result: ExecutionResult) -> None:
        """Store execution data for learning"""
        await self.perfect_recall.store_execution_data({
            "solution": solution,
            "result": result,
            "agent_id": self.agent_id,
            "timestamp": asyncio.get_event_loop().time()
        })
    
    async def _analyze_feedback(self, solution: Solution, result: ExecutionResult,
                               user_feedback: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze user feedback"""
        return {
            "satisfaction_score": user_feedback.get("satisfaction", 0.5),
            "improvement_suggestions": user_feedback.get("suggestions", []),
            "effectiveness_rating": user_feedback.get("effectiveness", 0.5)
        }
    
    async def _update_learning_model(self, feedback_analysis: Dict[str, Any]) -> None:
        """Update agent learning model"""
        # Update learning data based on feedback
        if "learning_patterns" not in self.learning_data:
            self.learning_data["learning_patterns"] = []
        
        self.learning_data["learning_patterns"].append(feedback_analysis)
    
    async def _update_learning_data(self, operation: str, data: Any) -> None:
        """Update learning data"""
        if "operations" not in self.learning_data:
            self.learning_data["operations"] = {}
        
        if operation not in self.learning_data["operations"]:
            self.learning_data["operations"][operation] = []
        
        self.learning_data["operations"][operation].append({
            "timestamp": asyncio.get_event_loop().time(),
            "data": data
        })
    
    async def _calculate_learning_score(self) -> float:
        """Calculate agent learning score"""
        if not self.learning_data.get("learning_patterns"):
            return 0.0
        
        patterns = self.learning_data["learning_patterns"]
        avg_satisfaction = sum(p.get("satisfaction_score", 0) for p in patterns) / len(patterns)
        return avg_satisfaction
    
    # Helper methods for analysis synthesis
    def _determine_problem_type(self, problem: Problem, context: Dict[str, Any]) -> str:
        """Determine the type of problem"""
        return "general"
    
    def _assess_complexity(self, parallel_results: List[Dict[str, Any]]) -> ProblemComplexity:
        """Assess problem complexity"""
        return ProblemComplexity.MODERATE
    
    def _determine_required_engines(self, creative_insights: Dict[str, Any]) -> List[str]:
        """Determine which engines are required"""
        return ["perfect_recall", "parallel_mind", "creative_engine"]
    
    def _calculate_confidence_score(self, parallel_results: List[Dict[str, Any]]) -> float:
        """Calculate confidence score"""
        return 0.8
    
    def _generate_recommendations(self, creative_insights: Dict[str, Any]) -> List[str]:
        """Generate recommendations"""
        return ["Consider multiple approaches", "Test thoroughly", "Monitor performance"]